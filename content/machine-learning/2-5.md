---
title: 2. 분류Classification

slug: 2-5

category: '피처feature 선택'

 ---

### 2. 5. 특징feature 선택: sklearn.feature_selection


**(이번 단원은 꼼꼼하게 공부하시지 않아도 괜찮습니다.)**

sklearn의 예제 데이터 외에 실제로 공공 데이터나 기관, 기업의 데이터로 예측 분석을 하는 경우, 열이면 아홉은 특징feature 데이터의 종류가 많습니다. 데이터 크기도 큰데, 특징feature도 많으니 분석도 느려지고, 유의미하지 않은 데이터까지 포함되어 어떤 특징feature가 중요한지 알기 어려워집니다.
 

sklearn의 feature_selection 모듈은 가장 중요한 특징feature 데이터만 선택해 특징 데이터의 종류를 줄여줍니다.


물론 분석가의 도메인 지식이 있다면 이 과정을 생략하거나, 간단하게 상관관계 분석만으로 건너 띄는 경우도 많습니다.


이번 파트에서는 도메인 지식이 전-혀 없지만, 공부할 시간은 없고 성능은 높여야 할 때 유용한 모듈에 대해 간단하게 설명합니다. 자세한 설명은 [sklearn 공식 API](https://scikit-learn.org/stable/modules/feature_selection.html)를 확인하세요:)






#### 2. 5. 1. 분산 기반: VarinaceThreshold

![VarinaceThreshold](../img/2-5-1_1_variance_threshold_lt.png)
\begin{equation}
{Var}[X] = p(1 - p)
\end{equation}


VarianceThreshold는 이름처럼 분산 임계값을 충족하지 않는 모든 피처를 제거합니다. 


기본적으로 분산이 0인 피처, 즉 모든 표본에서 동일한 값을 갖는 피처가 제거됩니다(y = 1 그래프를 떠올려 보세요!).


아래처럼 0과 1로 이루어진 boolean 피처로 이루어진 데이터 세트에서 첫번째 열column은 총 6개의 데이터 중 5개가 0이군요. 이러면 Variance Threshold 설정 값에 따라 극단값에 가깝다고 판단하면 열column이 삭제됩니다.




```python
from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], 
     [0, 1, 0], 
     [1, 0, 0], 
     [0, 1, 1], 
     [0, 1, 0], 
     [0, 1, 1]]


# 임계점Threshold를 0.8(80%)로 설정
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
sel.fit_transform(X)
>>> [[0 1]
     [1 0]
     [0 0]
     [1 1]
     [1 0]
     [1 1]]
```

```
from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], 
     [0, 1, 0], 
     [1, 0, 0], 
     [0, 1, 1], 
     [0, 1, 0], 
     [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
print(sel.fit_transform(X))
```



#### 2. 5. 2. Univariate Feature Selection(UFS)

Unvariate Feature Selection은 일변량 통계 검정을 기반으로 최적의 피처를 선택합니다.  
- SelectKBest: K 점 이상의 피처를 제외한 모든 피처 삭제
- SelectPercentile: 사용자가 정한 퍼센트 이상의 피처만 남기고 모든 피처 삭제
- 일변량 통계 검정 사용
    - SelectFpr: False positive rate
    - SelectFdr: False discovery rate
    - SelectFwe: Family wise error


회귀regression 분석인지, 분류classification 분석인지에 따라 SelectKBest, SelectPercentile의 하이퍼 파라미터가 다릅니다.
- 회귀: f_rregression, mutual_info_regression
- 분류: chi2, f_classif, mutual_info_classif


- 데이터가 희소행렬이라면?: chi2, mutual_info_regression, mutual_info_classif를 추천합니다.


이외에 자세한 내용은 [여기](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)를 참고하세요.


```python
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X, y = load_iris(return_X_y=True)
X.shape
>>> (150, 4)
```


```python
X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
X_new.shape
>>> (150, 2)
```

