---
title: 2. 분류Classification

slug: 2-4

category: '전처리preprocessing'

 ---

### 2. 4. 전처리: sklearn.preprocessing

#### `preprocessing` 데이터 전처리 모듈

 
데이터 사이언스, AI, Machine Learning 프로젝트의 80%는 데이터 전처리에 있다고 해도 과장이 아닙니다. 일단 적합한 데이터여야 분석이 가능하고, 잘 전처리 될 수록 분석 결과가 좋습니다.


DataFrame에서 피처를 옮기거나 삭제하는 작업은 Pandas로 할 수 있습니다. 하지만 어떤 피처의 대부분 값이 10~90 사이인데 갑자기 200짜리, -100짜리 이상치가 있는 데이터 전처리를 Pandas로 하려면 어떻게 해야할까요? 지금 떠오르기로는 범위를 나누고, 조건문을 걸어서 삭제해야할 것 같네요. 그런데 피처가 100개라고 생각해 보세요! 모든 피처 하나씩 전처리를 할 수는 없겠죠?


물론 log 변환이 더 좋은 전처리가 될 때도 있습니다. EDA를 통해 데이터를 잘 확인하고 적용해보세요.


sklearn에서는 preprocessing 모듈을 제공합니다. 여기서 우리는 Scaling을 위해 표준화Standardization 또는 정규화Normalization을 사용할 수 있습니다.


#### 2. 4. 1. `StandardScaler`
+ Standrad Scaler: 

![Standard Scaler](../img/2-4-1_1_standard_scaler_lt.png)
\begin{equation}
x_i^{'} = \frac{x_i-mean(x)}{stdev(x)}
\end{equation}


- **피처 표준화(standardization): 모든 데이터에 대한 피처의 평균을 빼고 분산으로 나눕니다.**
- 그래서 분산 스케일링이라고도 부른다.
- 스케일링된 피처의 결과는 **평균이 0, 분산이 1**입니다. 즉, 0을 중심으로 좌우로 적절히 데이터를 배치하는 전략이죠.
- **주의**: 표준화는 **가우시안 분포를 따른다고 가정**합니다. 만약 가우시안을 따르지 않을 경우 엉뚱한 결과가 나올 수 있습니다.
- 참고: NaN은 없는 것으로 취급됩니다. fitting에서 배제되며 transform에 남습니다.


```python
# sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)

from sklearn.preprocessing import StandardScaler
data = [[0, 0], [0, 0], [1, 1], [1, 1]]

## 스케일러 인스턴스화
scaler = StandardScaler()

## 데이터에 스케일러 적용
print(scaler.fit(data))
>>> StandardScaler()
```


```python
# 스케일링 된 데이터의 평균
StandardScaler()
print(scaler.mean_)
>>> [0.5 0.5]
```


```python
# StandardScaler로 스케일링 된 데이터
print(scaler.transform(data))
>>> [[-1. -1.]
     [-1. -1.]
     [ 1.  1.]
     [ 1.  1.]]
```




```python
## 1x2행렬 스케일링 한 결과 반환
print(scaler.transform([[2, 2]]))


>>> [[3. 3.]]
```


#### 2. 4. 2. `MinMaxScaler`


+ MinMax Scaler
![MinMax Scaler](../img/2-4-2_1_minmax_scaler_lt.png.png)

\begin{equation}
x_i^{'} = \frac{x_i-min(x)}{max(x)-min(x)}
\end{equation}


- MinMax Scaler는 제일 작은 값을 0, 제일 큰 값을 1로 변환합니다.
- 즉, 모든 데이터가 0과 1 사이에 있다고 가정
- **주의**: 데이터를 그대로 가져오므로 **이상치를 처리할 수 없습니다!**


```python
# sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True)

from sklearn.preprocessing import MinMaxScaler
data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler()
print(scaler.fit(data))

>>> MinMaxScaler()
```




```python
# 최대값
MinMaxScaler()
print(scaler.data_max_)
>>> [ 1. 18.]
```


```python
# MinMax Scaler로 Scaling된 데이터
print(scaler.transform(data))
>>> [[0.   0.  ]
     [0.25 0.25]
     [0.5  0.5 ]
     [1.   1.  ]]
```


```python
print(scaler.transform([[2, 2]]))
>>> [[1.5 0. ]]
```

![min_max_img](../img/2-4-2_2_minmax_scaler.jpg)
![min_max_img](https://t1.daumcdn.net/cfile/tistory/99D1C8435C95F75D1B)


[참조](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)


#### 2. 4. 3. `RobustScaler`
+ Robust Scaler

![robust_scaler_math](../img/2-4-3_1_robust_scaler_lt.png)
\begin{equation}
x_i^{'} = \frac{x_i-Q_2}{Q_3-Q_1}
\end{equation}


- Robust Scaler는 평균 대신 중앙값을 사용하고, IQR1, IQR4 값을 제거하고 25~75% 값만 사용합니다. 평균은 이상치의 영향을 많이 받지만, 중앙값은 순서로 정해지기 때문에 영향이 적습니다.
- 즉, 전체 범위의 50%만 사용해 양 **극단값을 제외**할 수 있습니다.
- **주의**: 이상치가 심하지 않고 몰려있는 데이터의 경우 많은 데이터가 삭제될 수 있으니 주의해야합니다.
- robust는 번역하면 '건장한'인데요. 통계학에서는 이상치를 제외해 아주 믿을만한 데이터라고 생각할 때 사용합니다.



```python
from sklearn.preprocessing import RobustScaler
X = [[ 1., -2.,  2.],
    [ -2.,  1.,  3.],
    [ 4.,  1., -2.]]
scaler = RobustScaler().fit(X)
scaler

>>> RobustScaler()
```


```python
RobustScaler()
print(scaler.transform(X))
>>> [[ 0.  -2.   0. ]
     [-1.   0.   0.4]
     [ 1.   0.  -1.6]]
```


#### 2. 4. 4. `MaxAbsScaler`


- MaxAbs Scaler
![robust_scaler_math](../img/2-4-4_1_maxabs_scaler_lt.png)

\begin{equation}
x_i^{'} = 2\big(\frac{x_i-x_{min}}{x_{max}-x_{min}}\big)
\end{equation}


- MaxAbs Sclaer는 **최대 절대값**과 **0**이 각각 1, 0이 되도록 스케일링합니다.
- 모든 데이터는 양수로 구성된 MinMaxScaler와 유사하게 동작합니다.
- **주의**: MinMax Scaler처럼 이상치에 민감(영향을 많이 받)습니다.
- NaN은 missing value로 취급되어 fitting되지 않고 trasform에 남습니다.

```python
from sklearn.preprocessing import MaxAbsScaler
X = [[ 1., -2.,  1.],
    [ 0.,  1.,  2.],
    [ 1.,  -1., 0.]]
transformer = MaxAbsScaler().fit(X)
transformer


>>> MaxAbsScaler()
```

```python
MaxAbsScaler()
scaler.transform(X)

## MinMax Scaler처럼 최대 최소 값 변환
>>> [[ 1. , -1. ,  0.5],
     [ 0. ,  0.5,  1. ],
     [ 1. , -0.5,  0. ]]
```


#### 2. 4. 5. `Normalizer`

- 피처(특성) 벡터의 모든 **유클리디안 길이가 1이 되도록 조정**합니다. (반지름 1인 원에 투영하는 느낌)
- 피처(특성)벡터의 길이나 크기와 상관 없이, **데이터의 방향이나 각도가 중요할 경우 사용**합니다.


```python
from sklearn.preprocessing import Normalizer
X = [[ 1., -2.,  1.],
    [ 0.,  1.,  2.],
    [ 1.,  -1., 0.]]
transformer = Normalizer().fit(X)
transformer

Normalizer()

transformer.transform(X)
>>>
array([[ 0.40824829, -0.81649658,  0.40824829],
       [ 0.        ,  0.4472136 ,  0.89442719],
       [ 0.70710678, -0.70710678,  0.        ]])
```

스케일러의 특징을 잘 모른다면 데이터 추출, 모델 선택, 하이퍼 파라미터 조정 등의 과정을 되짚어 가면서 최적의 모델을 찾기 위한 긴 여정을 떠나야 할지도 모릅니다. 그 전에 데이터에 맞는 전처리를 적용하시길 바랍니다.

